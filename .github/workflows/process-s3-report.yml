name: Process Consul Test Results

on:
  workflow_dispatch:
    inputs:
      s3_directory:
        description: 'S3 Directory Path (e.g., Result/consul/2025-12-10/00-42-50/)'
        required: true
        default: 'Result/consul/2025-12-10/00-42-50'
        type: string
  repository_dispatch:
    types: [s3-new-consul-directory]

env:
  S3_BUCKET: qstp-consul
  OUTPUT_DIR: ./allure-report
  SINGLE_FILE_DIR: ./allure-single-file
  SOURCE_DIR: ./s3-data/allure-results

jobs:
  generate-allure-report:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION || 'us-east-1' }}

      - name: Setup and debug
        id: setup
        run: |
          echo "GitHub Event: ${{ github.event_name }}"
          echo "GitHub Ref: ${{ github.ref }}"
          echo "GitHub SHA: ${{ github.sha }}"
          
          # Parse S3 directory from input
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            S3_DIR="${{ github.event.inputs.s3_directory }}"
            echo "Using manual input: $S3_DIR"
          elif [ "${{ github.event_name }}" == "repository_dispatch" ]; then
            S3_DIR="${{ github.event.client_payload.directory }}"
            echo "Using repository dispatch: $S3_DIR"
          else
            echo "Unknown trigger"
            exit 1
          fi
          
          # Remove trailing slash if present
          S3_DIR=${S3_DIR%/}
          echo "S3_DIR=$S3_DIR" >> $GITHUB_OUTPUT
          
          # Extract timestamp path for report
          # Convert Result/consul/2025-12-10/00-42-50 to 2025-12-10/00-42-50
          TIMESTAMP_PATH=$(echo "$S3_DIR" | sed 's|^Result/consul/||')
          echo "TIMESTAMP_PATH=$TIMESTAMP_PATH" >> $GITHUB_OUTPUT
          
          # Create safe identifier
          SAFE_ID=$(echo "$TIMESTAMP_PATH" | sed 's|/|_|g' | sed 's|-|_|g')
          echo "SAFE_ID=$SAFE_ID" >> $GITHUB_OUTPUT
          
          echo "Parsed S3 directory: $S3_DIR"
          echo "Report path: Report/consul/$TIMESTAMP_PATH"
          echo "Safe ID: $SAFE_ID"
          echo "Source directory: $SOURCE_DIR"
          
          # List S3 bucket to verify access
          echo "Listing S3 bucket root:"
          aws s3 ls "s3://$S3_BUCKET/" || true

      - name: Download test results from S3
        run: |
          echo "Downloading from: s3://$S3_BUCKET/${{ steps.setup.outputs.S3_DIR }}/"
          mkdir -p $SOURCE_DIR
          
          # Try to sync all files - use recursive to get all files
          echo "Syncing files from S3..."
          aws s3 sync "s3://$S3_BUCKET/${{ steps.setup.outputs.S3_DIR }}/" $SOURCE_DIR/ --exclude "*" --include "*.xml" --include "*.json"
          
          # Also try without filters if no files found
          if [ ! "$(find $SOURCE_DIR -name '*.xml' -o -name '*.json' | head -1)" ]; then
            echo "No XML/JSON files found with filters, trying without filters..."
            aws s3 sync "s3://$S3_BUCKET/${{ steps.setup.outputs.S3_DIR }}/" $SOURCE_DIR/
          fi
          
          # Check what was downloaded
          echo "Downloaded files in $SOURCE_DIR:"
          find $SOURCE_DIR -type f | head -30
          echo "Total files: $(find $SOURCE_DIR -type f 2>/dev/null | wc -l || echo 0)"
          
          # Show file types and counts
          echo ""
          echo "File type breakdown:"
          find $SOURCE_DIR -type f -name "*.xml" | wc -l | xargs echo "XML files:"
          find $SOURCE_DIR -type f -name "*.json" | wc -l | xargs echo "JSON files:"
          
          # List first few XML/JSON files
          echo ""
          echo "First few test files:"
          find $SOURCE_DIR -type f \( -name "*.xml" -o -name "*.json" \) | head -10
          
          # If still no files, list S3 contents
          if [ ! "$(find $SOURCE_DIR -type f | head -1)" ]; then
            echo ""
            echo "Listing files in S3 source:"
            aws s3 ls "s3://$S3_BUCKET/${{ steps.setup.outputs.S3_DIR }}/" --recursive || true
          fi

      - name: Install dependencies
        run: |
          # Update package list
          sudo apt-get update
          
          # Install Java 17 (Allure 2.24.0 works better with Java 17)
          sudo apt-get install -y openjdk-17-jre
          
          # Install curl and unzip
          sudo apt-get install -y curl unzip
          
          # Verify Java installation
          echo "Java version:"
          java -version

      - name: Install Allure
        run: |
          # Download Allure
          echo "Downloading Allure 2.24.0..."
          curl -L -o allure-2.24.0.zip https://github.com/allure-framework/allure2/releases/download/2.24.0/allure-2.24.0.zip
          
          # Extract Allure
          unzip -q allure-2.24.0.zip -d /opt/
          
          # Create symlink
          sudo ln -sf /opt/allure-2.24.0/bin/allure /usr/local/bin/allure
          
          # Verify installation
          echo "Allure version:"
          /opt/allure-2.24.0/bin/allure --version

      - name: Generate Allure Reports
        id: generate_report
        run: |
          # Check if we have any test result files
          echo "Checking for test files in $SOURCE_DIR..."
          
          XML_COUNT=$(find $SOURCE_DIR -name "*.xml" | wc -l)
          JSON_COUNT=$(find $SOURCE_DIR -name "*.json" | wc -l)
          TOTAL_FILES=$((XML_COUNT + JSON_COUNT))
          
          echo "Found $XML_COUNT XML files and $JSON_COUNT JSON files"
          
          if [ $TOTAL_FILES -gt 0 ]; then
            echo "âœ… Found test result files, generating reports..."
          
            # Clean output directories
            rm -rf $OUTPUT_DIR $SINGLE_FILE_DIR
            mkdir -p $OUTPUT_DIR $SINGLE_FILE_DIR
          
            # Generate multi-file report FIRST
            echo "Step 1: Generating multi-file report..."
            /opt/allure-2.24.0/bin/allure generate $SOURCE_DIR -o $OUTPUT_DIR --clean
          
            # Check if multi-file report was generated
            if [ -f "$OUTPUT_DIR/index.html" ]; then
              echo "âœ… Multi-file report generated successfully"
              echo "Multi-file report size: $(du -sh $OUTPUT_DIR)"
          
              # Count widgets in the report
              if [ -d "$OUTPUT_DIR/data" ]; then
                WIDGET_COUNT=$(ls -1 "$OUTPUT_DIR/data"/*.json 2>/dev/null | wc -l)
                echo "Widgets in report: $WIDGET_COUNT"
              fi
            else
              echo "âŒ Multi-file report generation failed"
              echo "Contents of $OUTPUT_DIR:"
              ls -la $OUTPUT_DIR/
            fi
          
            # Generate single-file report SECOND (from fresh source)
            echo ""
            echo "Step 2: Generating single-file report..."
            /opt/allure-2.24.0/bin/allure generate $SOURCE_DIR -o $SINGLE_FILE_DIR --single-file --clean
          
            # Check if single-file report was generated
            SINGLE_FILE=$(find $SINGLE_FILE_DIR -name "*.html" | head -1)
            if [ -n "$SINGLE_FILE" ]; then
              echo "âœ… Single-file report generated: $SINGLE_FILE"
              echo "Single-file size: $(du -h "$SINGLE_FILE" | cut -f1)"
            else
              echo "âŒ Single-file report generation failed"
              echo "Contents of $SINGLE_FILE_DIR:"
              ls -la $SINGLE_FILE_DIR/
            fi
          
            echo "HAS_REPORT=true" >> $GITHUB_OUTPUT
          else
            echo "âŒ No test result files found"
            echo "Contents of source directory:"
            find $SOURCE_DIR -type f | head -20
            echo "HAS_REPORT=false" >> $GITHUB_OUTPUT
          fi

      - name: Upload Reports to S3
        if: steps.generate_report.outputs.HAS_REPORT == 'true'
        run: |
          # Define report paths
          REPORT_BASE_PATH="Report/consul/${{ steps.setup.outputs.TIMESTAMP_PATH }}"
          
          echo "Uploading reports to S3..."
          echo "Base path: $REPORT_BASE_PATH"
          
          # Clean previous reports in S3
          echo "Cleaning previous reports..."
          aws s3 rm "s3://$S3_BUCKET/$REPORT_BASE_PATH/" --recursive --quiet || true
          
          # Upload multi-file report
          echo "Uploading multi-file report..."
          if [ -f "$OUTPUT_DIR/index.html" ]; then
            aws s3 sync "$OUTPUT_DIR/" "s3://$S3_BUCKET/$REPORT_BASE_PATH/multi-file/" \
              --quiet
            echo "âœ… Multi-file report uploaded"
          else
            echo "âŒ Multi-file report not found, skipping upload"
          fi
          
          # Upload single file report
          echo "Uploading single-file report..."
          SINGLE_FILE=$(find "$SINGLE_FILE_DIR" -name "*.html" | head -1)
          if [ -n "$SINGLE_FILE" ]; then
            SINGLE_FILE_NAME="allure-report-${{ steps.setup.outputs.SAFE_ID }}.html"
            echo "Uploading as: $SINGLE_FILE_NAME"
          
            # Copy the file with new name
            cp "$SINGLE_FILE" "$SINGLE_FILE_DIR/$SINGLE_FILE_NAME"
          
            # Upload to S3
            aws s3 cp "$SINGLE_FILE_DIR/$SINGLE_FILE_NAME" "s3://$S3_BUCKET/$REPORT_BASE_PATH/$SINGLE_FILE_NAME" \
              --quiet
          
            # Generate download URL
            SINGLE_FILE_URL="https://k8s-nginxs3v-nginxs3g-52e35e9339-1035610716.us-east-1.elb.amazonaws.com/$REPORT_BASE_PATH/$SINGLE_FILE_NAME"
            echo "SINGLE_FILE_URL=$SINGLE_FILE_URL" >> $GITHUB_ENV
            echo "âœ… Single-file report uploaded"
          else
            echo "âŒ Single-file report not found"
          fi
          
          # Create report URL for multi-file
          REPORT_URL="https://k8s-nginxs3v-nginxs3g-52e35e9339-1035610716.us-east-1.elb.amazonaws.com/$REPORT_BASE_PATH/multi-file/index.html"
          echo "REPORT_URL=$REPORT_URL" >> $GITHUB_ENV
          
          # Create a status file
          STATUS_FILE="./upload-status.json"
          cat > "$STATUS_FILE" << EOF
          {
            "status": "success",
            "timestamp": "${{ steps.setup.outputs.TIMESTAMP_PATH }}",
            "source_directory": "${{ steps.setup.outputs.S3_DIR }}",
            "files_found": $TOTAL_FILES,
            "multi_file_report": "$(if [ -f "$OUTPUT_DIR/index.html" ]; then echo "true"; else echo "false"; fi)",
            "single_file_report": "$(if [ -n "$(find $SINGLE_FILE_DIR -name '*.html' | head -1)" ]; then echo "true"; else echo "false"; fi)",
            "multi_file_url": "$REPORT_URL",
            "single_file_url": "$SINGLE_FILE_URL",
            "generated_at": "$(date -u +'%Y-%m-%dT%H:%M:%SZ')"
          }
          EOF
          
          aws s3 cp "$STATUS_FILE" "s3://$S3_BUCKET/$REPORT_BASE_PATH/status.json"
          
          echo ""
          echo "ðŸ“Š Upload Summary:"
          echo "Multi-file: $(if [ -f "$OUTPUT_DIR/index.html" ]; then echo 'âœ…'; else echo 'âŒ'; fi)"
          echo "Single-file: $(if [ -n "$(find $SINGLE_FILE_DIR -name '*.html' | head -1)" ]; then echo 'âœ…'; else echo 'âŒ'; fi)"

      - name: Create Artifacts
        if: steps.generate_report.outputs.HAS_REPORT == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: consul-report-${{ steps.setup.outputs.SAFE_ID }}
          path: |
            ${{ env.OUTPUT_DIR }}
            ${{ env.SINGLE_FILE_DIR }}
            ${{ env.SOURCE_DIR }}
          retention-days: 7

      - name: Output Results
        run: |
          echo ""
          echo "========================================"
          echo "ðŸ“‹ ALLURE REPORT GENERATION SUMMARY"
          echo "========================================"
          echo ""
          echo "Workflow: ${{ github.workflow }}"
          echo "Run ID: ${{ github.run_id }}"
          echo "Source: ${{ steps.setup.outputs.S3_DIR }}"
          echo ""
          
          if [ "${{ steps.generate_report.outputs.HAS_REPORT }}" = "true" ]; then
            echo "âœ… REPORT GENERATION COMPLETE"
            echo ""
          
            # Check multi-file report
            if [ -f "$OUTPUT_DIR/index.html" ]; then
              echo "ðŸ“Š MULTI-FILE REPORT: AVAILABLE"
              echo "   URL: ${{ env.REPORT_URL }}"
              echo "   Local: $OUTPUT_DIR/index.html"
              echo "   Size: $(du -sh $OUTPUT_DIR | cut -f1)"
            else
              echo "âŒ MULTI-FILE REPORT: FAILED"
            fi
          
            echo ""
          
            # Check single-file report
            SINGLE_FILE=$(find $SINGLE_FILE_DIR -name "*.html" | head -1)
            if [ -n "$SINGLE_FILE" ]; then
              echo "ðŸ“¥ SINGLE-FILE REPORT: AVAILABLE"
              echo "   URL: ${{ env.SINGLE_FILE_URL }}"
              echo "   File: $(basename $SINGLE_FILE)"
              echo "   Size: $(du -h "$SINGLE_FILE" | cut -f1)"
            else
              echo "âŒ SINGLE-FILE REPORT: FAILED"
            fi
          
            echo ""
            echo "ðŸ”— DIRECT DOWNLOAD COMMANDS:"
            echo "curl -LO '${{ env.SINGLE_FILE_URL }}'"
            echo ""
            echo "ðŸ“ S3 LOCATION:"
            echo "s3://$S3_BUCKET/Report/consul/${{ steps.setup.outputs.TIMESTAMP_PATH }}/"
          
          else
            echo "âš ï¸ NO REPORTS GENERATED"
            echo ""
            echo "Possible issues:"
            echo "1. No test files found in S3 directory"
            echo "2. Downloaded files are not in Allure format"
            echo "3. Check the 'Download test results from S3' step for details"
            echo ""
            echo "Debug commands:"
            echo "aws s3 ls s3://$S3_BUCKET/${{ steps.setup.outputs.S3_DIR }}/ --recursive"
          fi

      - name: Verify Reports
        if: steps.generate_report.outputs.HAS_REPORT == 'true'
        run: |
          echo "ðŸ” VERIFYING GENERATED REPORTS"
          echo "=============================="
          
          # Check multi-file report structure
          echo ""
          echo "Multi-file report structure ($OUTPUT_DIR):"
          if [ -d "$OUTPUT_DIR" ]; then
            find "$OUTPUT_DIR" -type f -name "*.html" -o -name "*.json" | head -10
            echo "Total files: $(find "$OUTPUT_DIR" -type f | wc -l)"
          
            # Check for essential files
            echo ""
            echo "Essential files check:"
            [ -f "$OUTPUT_DIR/index.html" ] && echo "âœ… index.html exists" || echo "âŒ index.html missing"
            [ -d "$OUTPUT_DIR/data" ] && echo "âœ… data/ directory exists" || echo "âŒ data/ directory missing"
            [ -d "$OUTPUT_DIR/plugins" ] && echo "âœ… plugins/ directory exists" || echo "âŒ plugins/ directory missing"
          fi
          
          # Check single-file report
          echo ""
          echo "Single-file report ($SINGLE_FILE_DIR):"
          if [ -d "$SINGLE_FILE_DIR" ]; then
            find "$SINGLE_FILE_DIR" -type f | head -10
            SINGLE_FILE=$(find "$SINGLE_FILE_DIR" -name "*.html" | head -1)
            if [ -n "$SINGLE_FILE" ]; then
              echo "Single file size: $(wc -c < "$SINGLE_FILE") bytes"
              # Check if file has content
              if [ $(wc -c < "$SINGLE_FILE") -gt 1000 ]; then
                echo "âœ… Single-file has sufficient content"
              else
                echo "âš ï¸ Single-file might be empty or too small"
              fi
            fi
          fi